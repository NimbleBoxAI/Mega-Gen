{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c18e80d4",
   "metadata": {},
   "source": [
    "# Semantic Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb94e746",
   "metadata": {},
   "source": [
    "In this notebook we follow a pipeline to extract relevant information from a data dump (generated by GPT).\n",
    "\n",
    "The **Pipeline** is as follows:\n",
    "* Read the Data\n",
    "* Extract the Topics (We work with topics since it is easier to prototype with. Full Sentences can also be used.)\n",
    "* Pre-Process the Data\n",
    "* Generate Semantic Embeddings using a SentenceTransformer Model\n",
    "* Perform K-Means Clustering\n",
    "\n",
    "The last step, *Perform K-Means Clustering*, itself has a few parts:\n",
    "* Optimize K for K-Means (this can be hard coded, or, as in the notebook, estimate from intertia)\n",
    "* If preferred, apply Dimensionality Reduction using TSNE or PCA (this has proved to defeat the point of semantic embeddings to a large extent)\n",
    "* Retrieve Cluster Assignments in the form of indexes\n",
    "* Use the indexes to get the real clusters as a List of List of Strings (This is also where lexical cluster refinement has been built into)\n",
    "* Print Clusters in a cohesive manner as well as the optimizations / datapoint filtering that has been done at multiple stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ea9b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import pylcs\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d484ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables for the pipeline\n",
    "\n",
    "red1 = None\n",
    "red2 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ce5d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Dataset\n",
    "\n",
    "with open('../data/I2_1_1000.txt','r') as txtfile:\n",
    "    biz_ideas = [line.rstrip('\\n') for line in txtfile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b30f593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic(s1:str):\n",
    "    \n",
    "    # Returns the topic extracted from the entire output.\n",
    "    \n",
    "    return s1[:s1.find('.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29176a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data:list, refer=None):\n",
    "    \n",
    "    # Applies three proprocessing steps: turns every item to lower case, selects only short strings, and eliminates duplicates\n",
    "    \n",
    "    data = [datum.lower() for datum in data]\n",
    "    processed = {}\n",
    "    \n",
    "    if refer:\n",
    "                \n",
    "        for i in range(len(data)):\n",
    "            \n",
    "            if len(data[i]) < 25:\n",
    "                processed[data[i]] = refer[i]\n",
    "        \n",
    "    else:\n",
    "        data_processed = [datum for datum in data if len(datum) < 25]\n",
    "        processed = dict(zip(data_processed, data_processed))\n",
    "        \n",
    "    global red1\n",
    "    red1 = (len(data) - len(processed.keys()))/len(data) * 100\n",
    "    \n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11a7c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap(s1:str, s2:str):\n",
    "    \n",
    "    # Returns the lexical overlap using longest common subsequence between two strings\n",
    "    \n",
    "    if len(s1) > len(s2):\n",
    "        return pylcs.lcs(s1,s2)/float(len(s1))\n",
    "    else:\n",
    "        return pylcs.lcs(s1,s2)/float(len(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ef91da",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_unprocessed = []\n",
    "\n",
    "for idea in biz_ideas:\n",
    "\n",
    "    topics_unprocessed.append(topic(idea))\n",
    "    \n",
    "ideas = dict(zip())\n",
    "\n",
    "processed = preprocess(topics_unprocessed,biz_ideas)\n",
    "\n",
    "df = pd.DataFrame(zip(processed.values(), processed.keys()))\n",
    "df.columns = ['Text','Topic']\n",
    "\n",
    "# Create a list of topics from the corpus\n",
    "\n",
    "topics = list(df['Topic'])\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dba5724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings using a pre-trained SentenceTransformer model\n",
    "\n",
    "model = SentenceTransformer('paraphrase-mpnet-base-v2')\n",
    "topics_embeddings_unprocessed = model.encode(topics_unprocessed)\n",
    "topics_embeddings = model.encode(topics)\n",
    "\n",
    "print(f'Shape of topic embeddings before pre-processing: {topics_embeddings_unprocessed.shape}')\n",
    "print(f'Shape of topic embeddings after pre-processing: {topics_embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311c39a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top K similar ideas (Pre-processed but unrefined)\n",
    "\n",
    "topic_query = 'Beauty'\n",
    "query_embedding = model.encode(topic_query)\n",
    "\n",
    "top_k = 5\n",
    "\n",
    "cos_scores = util.pytorch_cos_sim(query_embedding, topics_embeddings)[0]\n",
    "top_results = np.argpartition(-cos_scores, range(top_k))[0:top_k]\n",
    "\n",
    "print(\"Sentence:\", topic_query, \"\\n\")\n",
    "print(f'Top {top_k} most similar items in corpus:\\n')\n",
    "\n",
    "for idx in top_results[0:top_k]:\n",
    "    print(topics[idx], \"(Score: %.4f)\" % (cos_scores[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7b4814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering : KMeans\n",
    "\n",
    "def kmeans(num_clusters, data:list):\n",
    "    \n",
    "    from sklearn.cluster import KMeans\n",
    "        \n",
    "    clustering_model = KMeans(n_clusters = num_clusters)\n",
    "    clustering_model.fit(data)\n",
    "    cluster_assignment = clustering_model.labels_\n",
    "\n",
    "    return cluster_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f498ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_k(data:list):\n",
    "    \n",
    "    # This function is used to automatically determine the optimal value of K for K-Means Clustering\n",
    "    \n",
    "    from sklearn.cluster import KMeans\n",
    "    import math\n",
    "    \n",
    "    dists = []\n",
    "    grads = []\n",
    "    K = range(1,70)\n",
    "    iner_drop = 0\n",
    "    iner_last = 0\n",
    "    \n",
    "    for n in K:\n",
    "        k_model = KMeans(n_clusters = n)\n",
    "        k_model.fit(data)\n",
    "        dists.append(k_model.inertia_)\n",
    "        \n",
    "    \"\"\" # This was an experiment to try stopping the optimization early to save time based on KMeans Inertia\n",
    "        if n == 1:\n",
    "            iner_last = k_model.inertia_\n",
    "            grads.append(0)\n",
    "        else:\n",
    "            iner_drop = iner_last - k_model.inertia_\n",
    "            grad = iner_drop/iner_last\n",
    "            grads.append(grad)\n",
    "            iner_last = k_model.inertia_\n",
    "            \n",
    "    for x, (y,z) in enumerate(zip(dists,grads),1):\n",
    "        print(f'{x}. {y:.2f} {z:.4f}')\n",
    "    \"\"\"\n",
    "    def calc_dist(x1,y1,a,b,c):\n",
    "        return abs((a*x1 + b*y1 + c))/(math.sqrt(a**2 + b**2))\n",
    "        \n",
    "    a = dists[0] - dists[-1]\n",
    "    b = K[-1] - K[0]\n",
    "    c1 = K[0] * dists[-1]\n",
    "    c2 = K[-1] * dists[0]\n",
    "    c = c1 - c2\n",
    "        \n",
    "    dists_line = []\n",
    "\n",
    "    for k in range(K[-1]):\n",
    "        dists_line.append(calc_dist(K[k], dists[k], a, b, c))\n",
    "                \n",
    "    num_clusters = dists_line.index(max(dists_line))+1\n",
    "        \n",
    "    return num_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23772caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative Approach: DBScan\n",
    "\n",
    "def dbscan(data:list):\n",
    "    \n",
    "    from sklearn.cluster import DBSCAN\n",
    "\n",
    "    db_default = DBSCAN(eps = 0.0375, min_samples = 3).fit(data)\n",
    "    cluster_assignment = db_default.labels_\n",
    "\n",
    "    return cluster_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac13288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dims(data, alg='tsne', num_components=2):\n",
    "    \n",
    "    # Used to reduce dimensions using TSNE or PCA algorithms\n",
    "    \n",
    "    topics_red = None\n",
    "    \n",
    "    if alg == 'tsne':\n",
    "        \n",
    "        from sklearn.manifold import TSNE\n",
    "\n",
    "        topics_red = TSNE(n_components=num_components).fit_transform(data)\n",
    "        \n",
    "    elif alg == 'pca':\n",
    "        \n",
    "        from sklearn.decomposition import PCA\n",
    "        topics_red = PCA(n_components=num_components,svd_solver='full').fit_transform(data)\n",
    "            \n",
    "    return topics_red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954410ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(num_clusters, df):\n",
    "    \n",
    "    # Prepares and returns clusters as list of lists\n",
    "    \n",
    "    clusters = []\n",
    "    \n",
    "    for i in range(num_clusters):\n",
    "        \n",
    "        clust_sent = np.where(df['Cluster Assignment'] == i)\n",
    "        clust_points = []\n",
    "        \n",
    "        for k in clust_sent[0]:\n",
    "            \n",
    "            clust_points.append(df['Topic'][k])\n",
    "            \n",
    "        clusters.append(clust_points)\n",
    "    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d682cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_clusters(df, threshold=0.7):\n",
    "    \n",
    "    # Eliminates lexically similar items using Longest Common Subsequence\n",
    "    \n",
    "    refined_clusters = pd.DataFrame()\n",
    "    reductions = []\n",
    "\n",
    "    for i in range(num_clusters):\n",
    "\n",
    "        df_cluster = df.where(df['Cluster Assignment'] == i+1)\n",
    "\n",
    "        df_cluster.dropna(subset=['Text'], inplace=True)\n",
    "\n",
    "        refined_cluster = pd.DataFrame()\n",
    "\n",
    "        for j in range(df_cluster.shape[0]-1):\n",
    "            flag = True\n",
    "\n",
    "            for k in range(j+1,df_cluster.shape[0]):\n",
    "\n",
    "                overlap_ = overlap(df_cluster.iloc[j]['Topic'],df_cluster.iloc[k]['Topic'])\n",
    "\n",
    "                if overlap_ > 0.7:\n",
    "                    #print('Hit')\n",
    "                    flag = False\n",
    "                    break\n",
    "\n",
    "            if flag:\n",
    "                    refined_cluster = refined_cluster.append(df_cluster.iloc[j])\n",
    "\n",
    "        if df_cluster.shape[0]:\n",
    "                reductions.append((df_cluster.shape[0] - refined_cluster.shape[0]) / df_cluster.shape[0])\n",
    "                refined_clusters = refined_clusters.append(refined_cluster)\n",
    "\n",
    "    global red2\n",
    "    red2 = np.average(np.array(reductions))*100\n",
    "    \n",
    "    return refined_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd54df33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_clusters(num_clusters, df, n=10):\n",
    "    \n",
    "    # Prints clusters in a cohesive manner and also displays how much redundant data has been eliminated\n",
    "\n",
    "    for i in range(num_clusters):\n",
    "        \n",
    "        df_cluster = df.where(df['Cluster Assignment'] == i+1)\n",
    "        df_cluster.dropna(subset=['Text'], inplace=True)\n",
    "        display(df_cluster.head(n))\n",
    "        \n",
    "    global red1, red2\n",
    "    print(f'\\nData trimmed by {red1:.2f}% in preprocessing step, and by {red2:.2f}% in cluster refinement step.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c807b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# K-Means Pipeline\n",
    "\n",
    "# Define number of clusters or auto estimate optimum using intertia\n",
    "num_clusters = optimize_k(topics_embeddings)\n",
    "\n",
    "# Reduce Dimentions using TSNE or PCA\n",
    "topics_red = reduce_dims(topics_embeddings,alg='tsne',num_components=2)\n",
    "\n",
    "# Apply K-Means Clustering\n",
    "cluster_assignment = kmeans(num_clusters=num_clusters, data=topics_embeddings) # data=topics_embeddings, topics_red\n",
    "df['Cluster Assignment'] = cluster_assignment\n",
    "\n",
    "# Refine the clusters using LCS\n",
    "clusters = refine_clusters(df)\n",
    "\n",
    "# Print clusters cohesively\n",
    "print_clusters(num_clusters, clusters, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04d640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBScan Pipeline\n",
    "\n",
    "# Define number of clusters to show (note number of clusters is automatically determined)\n",
    "num_clusters = 75\n",
    "\n",
    "# Reduce Dimentions using TSNE\n",
    "topics_red = reduce_dims(topics_embeddings,alg='tsne',num_components=3)\n",
    "\n",
    "# Apply DBScan Clustering\n",
    "cluster_assignment = dbscan(data=topics_embeddings) # data=topics_embeddings, topics_red\n",
    "\n",
    "# Get the clusters\n",
    "clusters = get_clusters(num_clusters, cluster_assignment, topics)\n",
    "\n",
    "# Print clusters cohesively\n",
    "print_clusters(clusters, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8e400e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
