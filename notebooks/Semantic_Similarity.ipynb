{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c18e80d4",
   "metadata": {},
   "source": [
    "# Semantic Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb94e746",
   "metadata": {},
   "source": [
    "In this notebook we follow a pipeline to extract relevant information from a data dump (generated by GPT).\n",
    "\n",
    "The **Pipeline** is as follows:\n",
    "* Read the Data\n",
    "* Extract the Topics (We work with topics since it is easier to prototype with. Full Sentences can also be used.)\n",
    "* Pre-Process the Data\n",
    "* Generate Semantic Embeddings using a SentenceTransformer Model\n",
    "* Perform K-Means Clustering\n",
    "\n",
    "The last step, *Perform K-Means Clustering*, itself has a few parts:\n",
    "* Optimize K for K-Means (this can be hard coded, or, as in the notebook, estimate from intertia)\n",
    "* If preferred, apply Dimensionality Reduction using TSNE or PCA (this has proved to defeat the point of semantic embeddings to a large extent)\n",
    "* Retrieve Cluster Assignments in the form of indexes\n",
    "* Use the indexes to get the real clusters as a List of List of Strings (This is also where lexical cluster refinement has been built into)\n",
    "* Print Clusters in a cohesive manner as well as the optimizations / datapoint filtering that has been done at multiple stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62ea9b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import pylcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10d484ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables for the pipeline\n",
    "\n",
    "red1 = None\n",
    "red2 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82ce5d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Dataset\n",
    "\n",
    "with open('../data/I2_1_1000.txt','r') as txtfile:\n",
    "    biz_ideas = [line.rstrip('\\n') for line in txtfile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b30f593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic(s1:str):\n",
    "    \n",
    "    # Returns the topic extracted from the entire output.\n",
    "    \n",
    "    return s1[:s1.find('.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a29176a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data:list):\n",
    "    \n",
    "    # Applies three proprocessing steps: turns every item to lower case, selects only short strings, and eliminates duplicates\n",
    "    \n",
    "    len1 = len(data)\n",
    "    data = [datum.lower() for datum in data]\n",
    "    data = [datum for datum in data if len(datum) < 25]\n",
    "    data = [datum for datum in set(data)]\n",
    "    \n",
    "    global red1\n",
    "    red1 = (len1 - len(data))/len1 * 100\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b11a7c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap(s1:str, s2:str):\n",
    "    \n",
    "    # Returns the lexical overlap using longest common subsequence between two strings\n",
    "    \n",
    "    if len(s1) > len(s2):\n",
    "        return pylcs.lcs(s1,s2)/float(len(s1))\n",
    "    else:\n",
    "        return pylcs.lcs(s1,s2)/float(len(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b686641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laundries\n",
      "internet shoppes\n",
      "dog day care\n",
      "pet food business\n",
      "restaurant business\n"
     ]
    }
   ],
   "source": [
    "# Create a list of topics from the corpus\n",
    "\n",
    "topics_unprocessed = []\n",
    "\n",
    "for idea in biz_ideas:\n",
    "\n",
    "    topics_unprocessed.append(topic(idea))\n",
    "\n",
    "topics = preprocess(topics_unprocessed)\n",
    "\n",
    "for topic_ in topics[:5]:\n",
    "    print(topic_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dba5724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings using a pre-trained SentenceTransformer model\n",
    "\n",
    "model = SentenceTransformer('paraphrase-mpnet-base-v2')\n",
    "topics_embeddings_unprocessed = model.encode(topics_unprocessed)\n",
    "topics_embeddings = model.encode(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ef69da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of topic embeddings before pre-processing: (1000, 768)\n",
      "Shape of topic embeddings after pre-processing: (487, 768)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of topic embeddings before pre-processing: {topics_embeddings_unprocessed.shape}')\n",
    "print(f'Shape of topic embeddings after pre-processing: {topics_embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "311c39a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Beauty Parlor \n",
      "\n",
      "Top 5 most similar items in corpus:\n",
      "\n",
      "beauty salon (Score: 0.9091)\n",
      "a beauty salon (Score: 0.9033)\n",
      "beauty shop (Score: 0.8997)\n",
      "beauty salons (Score: 0.8859)\n",
      "hair care salon (Score: 0.7918)\n"
     ]
    }
   ],
   "source": [
    "# Top K similar ideas (Pre-processed but unrefined)\n",
    "\n",
    "topic_query = 'Beauty Parlor'\n",
    "query_embedding = model.encode(topic_query)\n",
    "\n",
    "top_k = 5\n",
    "\n",
    "cos_scores = util.pytorch_cos_sim(query_embedding, topics_embeddings)[0]\n",
    "top_results = np.argpartition(-cos_scores, range(top_k))[0:top_k]\n",
    "\n",
    "print(\"Sentence:\", topic_query, \"\\n\")\n",
    "print(f'Top {top_k} most similar items in corpus:\\n')\n",
    "\n",
    "for idx in top_results[0:top_k]:\n",
    "    print(topics[idx], \"(Score: %.4f)\" % (cos_scores[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e7b4814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering : KMeans\n",
    "\n",
    "def kmeans(num_clusters, data:list):\n",
    "    \n",
    "    from sklearn.cluster import KMeans\n",
    "        \n",
    "    clustering_model = KMeans(n_clusters = num_clusters)\n",
    "    clustering_model.fit(data)\n",
    "    cluster_assignment = clustering_model.labels_\n",
    "\n",
    "    return cluster_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25f498ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_k(data:list):\n",
    "    \n",
    "    # This function is used to automatically determine the optimal value of K for K-Means Clustering\n",
    "    \n",
    "    from sklearn.cluster import KMeans\n",
    "    import math\n",
    "    \n",
    "    dists = []\n",
    "    K = range(1,70)\n",
    "        \n",
    "    for n in K:\n",
    "        k_model = KMeans(n_clusters = n)\n",
    "        k_model.fit(data)\n",
    "        dists.append(k_model.inertia_)\n",
    "        \n",
    "    def calc_dist(x1,y1,a,b,c):\n",
    "        return abs((a*x1 + b*y1 + c))/(math.sqrt(a**2 + b**2))\n",
    "        \n",
    "    a = dists[0] - dists[-1]\n",
    "    b = K[-1] - K[0]\n",
    "    c1 = K[0] * dists[-1]\n",
    "    c2 = K[-1] * dists[0]\n",
    "    c = c1 - c2\n",
    "        \n",
    "    dists_line = []\n",
    "\n",
    "    for k in range(K[-1]):\n",
    "        dists_line.append(calc_dist(K[k], dists[k], a, b, c))\n",
    "            \n",
    "    num_clusters = dists_line.index(max(dists_line))+1\n",
    "        \n",
    "    return num_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23772caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative Approach: DBScan\n",
    "\n",
    "def dbscan(data:list):\n",
    "    \n",
    "    from sklearn.cluster import DBSCAN\n",
    "\n",
    "    db_default = DBSCAN(eps = 0.0375, min_samples = 3).fit(data)\n",
    "    cluster_assignment = db_default.labels_\n",
    "\n",
    "    return cluster_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dac13288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dims(data, alg='tsne', num_components=2):\n",
    "    \n",
    "    # Used to reduce dimensions using TSNE or PCA algorithms\n",
    "    \n",
    "    topics_red = None\n",
    "    \n",
    "    if alg == 'tsne':\n",
    "        \n",
    "        from sklearn.manifold import TSNE\n",
    "\n",
    "        topics_red = TSNE(n_components=num_components).fit_transform(data)\n",
    "        \n",
    "    elif alg == 'pca':\n",
    "        \n",
    "        from sklearn.decomposition import PCA\n",
    "        topics_red = PCA(n_components=num_components,svd_solver='full').fit_transform(data)\n",
    "            \n",
    "    return topics_red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a27f5dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_clusters(clusters:list, threshold=0.7):\n",
    "    \n",
    "    # Eliminates lexically similar items using Longest Common Subsequence\n",
    "    \n",
    "    refined_clusters = []\n",
    "    reductions = []\n",
    "    \n",
    "    for cluster in clusters:\n",
    "        \n",
    "        refined_cluster = []\n",
    "\n",
    "        for i in range(len(cluster)-1):\n",
    "            flag = True\n",
    "            \n",
    "            for j in range(i+1,len(cluster)):\n",
    "                \n",
    "                if overlap(cluster[i],cluster[j]) > threshold:\n",
    "                    flag = False\n",
    "                    break\n",
    "                    \n",
    "            if flag:\n",
    "                refined_cluster.append(cluster[i])\n",
    "        \n",
    "        if len(cluster):\n",
    "            reductions.append((len(cluster) - len(refined_cluster)) / len(cluster))\n",
    "            refined_clusters.append(refined_cluster)\n",
    "    \n",
    "    global red2\n",
    "    red2 = np.average(np.array(reductions))*100\n",
    "    \n",
    "    return refined_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d82883bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(num_clusters, cluster_assignment, topics):\n",
    "    \n",
    "    # Prepares and returns clusters as list of lists\n",
    "    \n",
    "    clusters = []\n",
    "    \n",
    "    for i in range(num_clusters):\n",
    "        \n",
    "        clust_sent = np.where(cluster_assignment == i)\n",
    "        clust_points = []\n",
    "        \n",
    "        for k in clust_sent[0]:\n",
    "            \n",
    "            clust_points.append(topics[k])\n",
    "            \n",
    "        clusters.append(clust_points)\n",
    "    \n",
    "    clusters = refine_clusters(clusters)\n",
    "    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b31dcf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_clusters(clusters, n=10):\n",
    "    \n",
    "    # Prints clusters in a cohesive manner and also displays how much redundant data has been eliminated\n",
    "\n",
    "    for i in range(len(clusters)):\n",
    "        print()\n",
    "        print(f'Cluster {i + 1} contains:')\n",
    "        \n",
    "        for j in range(min(n,len(clusters[i]))):\n",
    "            print(f'- {clusters[i][j]}')\n",
    "    \n",
    "    global red1, red2\n",
    "    print(f'\\nData trimmed by {red1:.2f}% in preprocessing step, and by {red2:.2f}% in cluster refinement step.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c807b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# K-Means Pipeline\n",
    "\n",
    "# Define number of clusters or auto estimate optimum using intertia\n",
    "num_clusters = optimize_k(topics_embeddings)\n",
    "\n",
    "# Reduce Dimentions using TSNE or PCA\n",
    "topics_red = reduce_dims(topics_embeddings,alg='tsne',num_components=2)\n",
    "\n",
    "# Apply K-Means Clustering\n",
    "cluster_assignment = kmeans(num_clusters=num_clusters, data=topics_embeddings) # data=topics_embeddings, topics_red\n",
    "\n",
    "# Get the clusters\n",
    "clusters = get_clusters(num_clusters, cluster_assignment, topics)\n",
    "\n",
    "# Print clusters cohesively\n",
    "print_clusters(clusters, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04d640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBScan Pipeline\n",
    "\n",
    "# Define number of clusters to show (note number of clusters is automatically determined)\n",
    "num_clusters = 75\n",
    "\n",
    "# Reduce Dimentions using TSNE\n",
    "topics_red = reduce_dims(topics_embeddings,alg='tsne',num_components=3)\n",
    "\n",
    "# Apply DBScan Clustering\n",
    "cluster_assignment = dbscan(data=topics_embeddings) # data=topics_embeddings, topics_red\n",
    "\n",
    "# Get the clusters\n",
    "clusters = get_clusters(num_clusters, cluster_assignment, topics)\n",
    "\n",
    "# Print clusters cohesively\n",
    "print_clusters(clusters, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8e400e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
