{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13bbba01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import hashlib\n",
    "import warnings\n",
    "import numpy as np\n",
    "from time import time\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14ebc2bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded ..., moving to GPU\n",
      "CPU times: user 1min 16s, sys: 17 s, total: 1min 33s\n",
      "Wall time: 2min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "name = \"EleutherAI/gpt-neo-2.7B\"\n",
    "# get the model and tokenizer, \n",
    "# EleutherAI/gpt-neo-2.7B = 9.9GB compressed\n",
    "# gpt2-xl (1.5Bn Params) = 6.7GB compressed\n",
    "# always cache these models, reduces useless bandwidth.\n",
    "tokenizer = AutoTokenizer.from_pretrained(name, cache_dir = \"../../hf_cache/\")\n",
    "model = AutoModelForCausalLM.from_pretrained(name, cache_dir = \"../../hf_cache/\")\n",
    "print(\"Model loaded ..., moving to GPU\")\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else \"CPU\"\n",
    "model = model.to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4e8219a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder():\n",
    "  try:\n",
    "    return os.path.dirname(os.path.realpath(__file__))\n",
    "  except:\n",
    "    return \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdf4c4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def md5(t):\n",
    "  return hashlib.md5(t.encode(\"utf-8\")).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebfa10c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class History():\n",
    "  \"\"\"Class that records the responses given by GPT so you don't have to waste\n",
    "  time copy pasting things into some notion file. Data is stored as a dictionary in\n",
    "  a pickle object and each item is a dictionary with keys like this:\n",
    "  {\n",
    "    \"b10a8db164e0754105b7a99be72e3fe5\": {\n",
    "      \"prompt\": \"Hello world\",\n",
    "      \"args\": {\"n\": 10, \"r\": 2},\n",
    "      \"responses\": [\n",
    "        'Hello world!\" \"Hello?\" \"Yeah, I'm in',\n",
    "        'Hello world\" command, that is, the command that'\n",
    "      ]\n",
    "    }\n",
    "    ...\n",
    "  }\n",
    "  \n",
    "  NOTE: we only create keys based on the prompts and not on generation args so\n",
    "        args are cached only for first time.\n",
    "  \"\"\"\n",
    "  def __init__(self, loc = None):\n",
    "    self.loc = os.path.join(folder(), 'gpt_history.p') if loc is None else loc\n",
    "    hist = {}\n",
    "    keys = []\n",
    "    if os.path.exists(self.loc):\n",
    "      # create the history list and load data from location\n",
    "      st = time()\n",
    "      with open(self.loc, \"rb\") as f:\n",
    "        hist = pickle.load(f)\n",
    "      keys = list(hist.keys())\n",
    "      warnings.warn(f\"Loading took: {time() - st:.2f}s\")\n",
    "    else:\n",
    "      warnings.warn(f\"No file found at: {self.loc}! Will create a new one.\")\n",
    "    self.hist = hist\n",
    "    self.keys = keys\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.hist)\n",
    "\n",
    "  def __getitem__(self, i):\n",
    "    return self.hist[i]\n",
    "\n",
    "  def __repr__(self):\n",
    "    n = -100; m = 70\n",
    "    keys = self.keys[n:][::-1]\n",
    "    _t = f\"<gpt.History :: {len(self)} >\\n\"\n",
    "    _t += \"-\" * 70 + \"\\n\"\n",
    "    print(keys)\n",
    "    for i,x in enumerate(keys):\n",
    "      _t += f\"[{i:03d} :: {len(self.hist[x]['responses']):04d}] {self.hist[x]['prompt'][:60]}\" + \"\\n\"\n",
    "    return _t\n",
    "  \n",
    "  def save(self):\n",
    "    # save the pickle file\n",
    "    with open(self.loc, \"wb\") as f:\n",
    "      pickle.dump(self.hist, f)\n",
    "  \n",
    "  def add_item(self, x):\n",
    "    assert isinstance(x, Response), \"Input needs to be a gpt.Response object\"\n",
    "    key = md5(x.prompt)\n",
    "    if key in self.keys:\n",
    "      # this prompt already exists in the history, we don't update the gen kwargs\n",
    "      n = self.hist[key]\n",
    "      n[\"responses\"] = list(set(self.hist[key]['responses'] + x.decoded))\n",
    "    else:\n",
    "      n = {\n",
    "        \"prompt\": x.prompt,\n",
    "        \"responses\": x.decoded,\n",
    "        \"args\": x.gen_kwargs\n",
    "      }\n",
    "      self.keys.append(key)\n",
    "    self.hist[key] = n\n",
    "    \n",
    "    self.save() # update the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c866974",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Response():\n",
    "  \"\"\"Class that makes getting generated results chill, simply `print(out)`\"\"\"\n",
    "  def __init__(self, prompt, out, t, gen_kwargs):\n",
    "    self.prompt = prompt # need to add for history\n",
    "    self.gen_kwargs = gen_kwargs # need to add for history\n",
    "    self.t = t\n",
    "    \n",
    "    # get the generated hidden_states/attentions/strings\n",
    "    self.sequences = out.sequences.cpu().tolist()\n",
    "    self.scores = [x.cpu().numpy() for x in out.scores] if out.scores != None else None\n",
    "    self.hidden_states = [\n",
    "      [y.cpu().numpy() for y in x]\n",
    "      for x in out.hidden_states\n",
    "    ] if out.hidden_states != None else None\n",
    "    self.attentions = [\n",
    "      [y.cpu().numpy() for y in x]\n",
    "      for x in out.attentions\n",
    "    ] if out.attentions != None else None\n",
    "    self.decoded = self.t.batch_decode(self.sequences, skip_special_tokens = True)\n",
    "\n",
    "  def __repr__(self):\n",
    "    str_ = \"\"\n",
    "    for x in self.decoded:\n",
    "      str_ += x + \"\\n\"\n",
    "      str_ += \"-\"* 70 + \"\\n\"\n",
    "    return str_\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.decoded)\n",
    "\n",
    "  def __getitem__(self, i):\n",
    "    return self.decoded[i]\n",
    "  \n",
    "  def __iter__(self):\n",
    "    for x in self.decoded:\n",
    "      yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bad8286",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class GPT():\n",
    "  \"\"\"Make GPT a first class object and using it as simple as possible.\n",
    "  First define the model and tokenizer\n",
    "  \n",
    "  >>> device = torch.device(\"cuda:0\") if torch.cuda.is_available() else \"CPU\"\n",
    "  >>> tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "  >>> model = AutoModelForCausalLM.from_pretrained(name, cache_dir = \"../hf-cache/\").eval().to(device)\n",
    "  Make GPT wrapper: output is `Response`, a class with __repr__ overloaded so print gives the generation\n",
    "  >>> gpt = GPT(model, tokenizer)\n",
    "  >>> out = gpt(\"Hello world\", n = 10, r = 2)\n",
    "  >>> out\n",
    "  ... Hello world!\" \"Hello?\" \"Yeah, I'm in\n",
    "      ----------------------------------------------------------------------\n",
    "      Hello world\" command, that is, the command that\n",
    "      ----------------------------------------------------------------------\n",
    "  \"\"\"\n",
    "  def __init__(self, model, tokenizer, history_loc = None):\n",
    "    self.model = model\n",
    "    self.tokenizer = tokenizer\n",
    "    self.eot_id = tokenizer.eos_token_id\n",
    "    self.device = self.model.device\n",
    "    \n",
    "    self.history = History()\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def __call__(\n",
    "    self,\n",
    "    prompt: str,\n",
    "    n: int = 16, # number of tokens\n",
    "    r: int = 1, # number of sequences\n",
    "    do_sample = True,\n",
    "    temp = 0.9,\n",
    "    top_p = 0.9,\n",
    "    top_k = None,\n",
    "    output_scores = None,\n",
    "    output_hidden_states = None,\n",
    "    output_attentions = None,\n",
    "    stop_sequence = None,\n",
    "    return_response = True,\n",
    "    **gen_kwargs\n",
    "  ):\n",
    "    \"\"\" __call__ overloader initialises the model.generate() function. We emphasise\n",
    "    a lot more on most powerful arguments, but you can always pass custom kwargs through\n",
    "    `gen_kwargs`. As you can see that we have not added many beam-search related arguments.\n",
    "    Args:\n",
    "      prompt (str): prompt string, tokens will be generated in continuation\n",
    "      n (int, optional): number of tokens to return\n",
    "      r (int, optional): number of sequences to return\n",
    "      temp (float, optional): sampling temperature\n",
    "      top_p (float, optional): tokens whose probability adds up to this are considered\n",
    "      top_k (int, optional): top-k tokens to consider for each distribution\n",
    "      output_scores (bool, optional): output scores for each generted token, returns shape `[r,n]`\n",
    "      output_hidden_states (bool, optional): output the hidden states of the generation, returns shape `[r,n+1,...]`\n",
    "      output_attentions (bool, optional): Whether or not to return the attentions tensors of all attention layers\n",
    "      stop_sequence (str, optional): Stop generation once the first token of this string is achieved\n",
    "      return_response (bool, optional): To parse the generated dictionary to `Response` class\n",
    "      gen_kwargs (dict, optional): any extra arguments to pass to the model.generate() method\n",
    "    Returns:\n",
    "      if return_response:\n",
    "       Response instance\n",
    "      else:\n",
    "       model.generate() output\n",
    "    \"\"\"\n",
    "    t = self.tokenizer\n",
    "    m = self.model\n",
    "    \n",
    "    # tokenize the input prompt and stop token if provided\n",
    "    input_ids = t(prompt, return_tensors = \"pt\")[\"input_ids\"].to(self.device)\n",
    "    if stop_sequence is not None:\n",
    "      eos_token_id = t(stop_sequence)[\"input_ids\"][0]\n",
    "    else:\n",
    "      eos_token_id = self.eot_id\n",
    "      \n",
    "    # generate the items\n",
    "    out = m.generate(\n",
    "      input_ids,\n",
    "      max_length = len(input_ids[0]) + n,\n",
    "      temperature = temp,\n",
    "      top_p=top_p,\n",
    "      top_k=top_k,\n",
    "      num_return_sequences=r,\n",
    "      pad_token_id = self.eot_id,\n",
    "      output_scores = output_scores,\n",
    "      output_hidden_states = output_hidden_states,\n",
    "      output_attentions = output_attentions,\n",
    "      do_sample = do_sample,\n",
    "      return_dict_in_generate = True,\n",
    "      eos_token_id = eos_token_id,\n",
    "      **gen_kwargs\n",
    "    )\n",
    "    \n",
    "    # return items or \n",
    "    if return_response:\n",
    "      x = Response(prompt, out, t, {\n",
    "        \"n\": n, \"r\": r,                                \n",
    "        \"do_sample\": do_sample,\n",
    "        \"temp\": temp,\n",
    "        \"top_p\": top_p,\n",
    "        \"top_k\": top_k,\n",
    "        **gen_kwargs\n",
    "      })\n",
    "      self.history.add_item(x)\n",
    "      return x\n",
    "    else:\n",
    "      return out\n",
    "\n",
    "  def classify(\n",
    "    self,\n",
    "    prompt: str,\n",
    "    labels: list,\n",
    "    softmax_temp = 0.9,\n",
    "    add_unknown = False,\n",
    "    **gen_kwargs,\n",
    "  ) -> dict:\n",
    "    \"\"\"Perform classification directly.\n",
    "    NOTE: ensure that first tokens in labels are not the same.\n",
    "    Args:\n",
    "      prompt (str): prompt string to be given as input\n",
    "      labels (list): list of strings that are labels\n",
    "      gen_kwargs (dict, optional): extra arguments to be passed for generation\n",
    "      softmax_temp (float, optional): temprature for scoring labels. Defaults to 0.9.\n",
    "      add_unknown (bool, optional): adds an extra \"Unknown\" label. Defaults to False.\n",
    "    Returns:\n",
    "      dict: values are 0. if model returns 'nan'\n",
    "    \"\"\"\n",
    "    # we will use the same format that OpenAI uses for GPT-3\n",
    "    # read: https://beta.openai.com/docs/guides/classifications\n",
    "    # We normalize all labels by `label.strip().lower().capitalize()` at the API\n",
    "    # backend. Thus corresponding output labels are always capitalized.\n",
    "    unq_options = set([x.strip().lower().capitalize() for x in labels])\n",
    "    unq_options = sorted(list(unq_options))\n",
    "\n",
    "    # each label must have a distinct first token, because classification\n",
    "    # works by looking only one step ahead. Also encode the labels with extra\n",
    "    # white space prepended.\n",
    "    label_ids = [self.tokenizer.encode(\" \" + x)[0] for x in unq_options]\n",
    "    \n",
    "    out = self(prompt, n = 1, r = 1, output_scores = True, **gen_kwargs, return_response = False)\n",
    "    logits = out.scores[0][0]\n",
    "    logits = (logits / softmax_temp)[label_ids].softmax(-1).cpu()\n",
    "    logits = logits.numpy()\n",
    "\n",
    "    scores = {o:i for o,i in zip(unq_options, logits)}\n",
    "    \n",
    "    # naaaaan - check\n",
    "    scores = {k: 0. if np.isnan(l) else l for k,l in scores.items()}\n",
    "\n",
    "    if add_unknown:\n",
    "      # fill the Probability for the special \"Unknown\" token\n",
    "      scores[\"Unknown\"] = 1 - sum(scores.values())\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13776a90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-20c9e9125412>:30: UserWarning: Loading took: 0.00s\n",
      "  warnings.warn(f\"Loading took: {time() - st:.2f}s\")\n"
     ]
    }
   ],
   "source": [
    "gpt = GPT(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea39547",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "prompt = \"\"\"Here we are trying to ideate Server Side Cloud Cost Optimisation offerings for a tech startup company\n",
    "\n",
    "Cloud provides the ability to scale systems to never before heard sizes and complexity while solving some of the most pressing issues of the time.\n",
    "\n",
    "Cloud costs are now the biggest expense of any IT company growing exponentially over the past 5 years, with no likely way to slow things down. This means that organizations will have to be much smarter when it comes to using the cloud and it's services.\n",
    "\n",
    "1. We will be building systems that take in bills from the cloud providers or consume billing information from an API and provide use OCR to extract information and provide optimal solutions for managing the resources.\n",
    "2. We intend to build personalised solutions for different organizations that can help better manage cloud services. This would be built using the data obtained from public sources as well as from the billing and other information from the organizations.\n",
    "3.\"\"\"\n",
    "\n",
    "total_responses = []\n",
    "\n",
    "for i in range(200):\n",
    "    \n",
    "    out = gpt(prompt, n=100, temp=0.98, top_p=1, stop_sequence='4', r=5)\n",
    "    responses = [response[len(prompt)+1:].split('\\n')[0].strip() for response in out.decoded]\n",
    "    \n",
    "    total_responses.extend(responses)\n",
    "\n",
    "for i,response in enumerate(total_responses[:5],1):\n",
    "    print(f'{i}. {response}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beab3dc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('../data/I1_1_1000.txt','w') as txtfile:\n",
    "    r = '\\n'.join(total_responses)\n",
    "    txtfile.write(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a90b72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "prompt = \"\"\"Here we are trying to list some business ideas that would not cost too much\n",
    "\n",
    "Businesses that have high profit margins may cost a little more, but can still be afforable on a loan.\n",
    "Businesses that have lower profit margins should almost always cost less because a loan would not be feasible.\n",
    "\n",
    "1. Ice Cream Truck. One could purchase an ice cream truck for as low as $20000 and get it up and running. Ice creams are quite profitable in summer seasons, but are not profitable during winter months.\n",
    "2. Woodworking Shop. One could set up a woodworking store for as low as $6000, excluding rent, taking the total up to about $12000 to start up. These are not too profitable, but the low start up costs make it a lucrative option.\n",
    "3. Cleaning and Care services. This business does not necessarily need a store, which saves money. For as low as $3000, one can get all the required equipment and start their business. This option is moderately profitable, but demand depends on location.\n",
    "4.\"\"\"\n",
    "\n",
    "total_responses = []\n",
    "\n",
    "for i in range(200):\n",
    "    \n",
    "    out = gpt(prompt, n=100, temp=0.98, top_p=1, stop_sequence='5', r=5)\n",
    "    responses = [response[len(prompt)+1:].split('\\n')[0].strip() for response in out.decoded]\n",
    "    \n",
    "    total_responses.extend(responses)\n",
    "\n",
    "for i,response in enumerate(total_responses[:5],1):\n",
    "    print(f'{i}. {response}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8ec551",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('../data/I2_1_1000.txt','w') as txtfile:\n",
    "    r = '\\n'.join(total_responses)\n",
    "    txtfile.write(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25231d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Consolidate operations. The company could take on a few new businesses, like selling insurance, auto repair, and similar, and then take over operations of these businesses in its place.\n",
      "\n",
      "2. Consolidate. Consolidating the company means eliminating the divisional or branch network. This is more time consuming, but can help boost the revenue considerably.\n",
      "\n",
      "3. Consider increasing dividends. A company with high dividends can increase the earnings of a dividend and increase the number of shares outstanding.\n",
      "\n",
      "4. Refinance an old debt. Even if a company has little debt in the bank, it can still leverage itself by refinanceing a portion of its existing debt with other banks. After refinancing, it can borrow more money if the company requires additional capital to keep it afloat.\n",
      "\n",
      "5. Cut down on staff and administration costs.\n",
      "\n",
      "CPU times: user 1h 3min 43s, sys: 55 s, total: 1h 4min 38s\n",
      "Wall time: 1h 4min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = \"\"\"A Company needs to cut costs due to financial strain. Here are some things the company management should consider:\n",
    "\n",
    "1. Cut marketing costs. Cutting Marketing costs will reduce inflow of potential new clients, but will save money in the immediate term while the company floats on its regular clientele.\n",
    "2. Layoff employees. The company could consider reducing its workforce temporarily by 20-30%, keeping the core team and main operations personnel.\n",
    "3. Take a business loan. After assessing the liabilities it already has, the company could request a new line of credit which would help it stay afloat for a certain time.\n",
    "4.\"\"\"\n",
    "\n",
    "total_responses = []\n",
    "\n",
    "for i in range(200):\n",
    "    \n",
    "    out = gpt(prompt, n=100, temp=0.98, top_p=1, stop_sequence='5', r=5)\n",
    "    responses = [response[len(prompt)+1:].split('\\n')[0].strip() for response in out.decoded]\n",
    "    \n",
    "    total_responses.extend(responses)\n",
    "\n",
    "for i,response in enumerate(total_responses[:5],1):\n",
    "    print(f'{i}. {response}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f1d220",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/I3_1_1000.txt','w') as txtfile:\n",
    "    r = '\\n'.join(total_responses)\n",
    "    txtfile.write(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML and DL (PyTorch Only) with CUDA 11.1",
   "language": "python",
   "name": "cuda111"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
